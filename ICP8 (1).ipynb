{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LRyzXyQeiXGt"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkContext and SparkSession\n",
        "context = SparkContext(\"local\", \"Big Data Assignment ICP\")\n",
        "session = SparkSession(context)"
      ],
      "metadata": {
        "id": "Dwhf7aQVidWm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Produce RDD with List of first 15 natural numbers\n",
        "numbers_rdd = context.parallelize(range(1, 16))\n",
        "print(\"RDD with first 15 natural numbers:\", numbers_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy-zBLiDieOF",
        "outputId": "4d0f7cf4-6540-4b52-8cb0-18a49198facc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD with first 15 natural numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Show the elements and number of partitions in RDD\n",
        "print(\"Elements in RDD:\", numbers_rdd.collect())\n",
        "print(\"Number of partitions:\", numbers_rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozPds_B-iiaT",
        "outputId": "dc98490f-501f-426d-9b66-010b07527dec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elements in RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "Number of partitions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Return the first element in the RDD\n",
        "initial_element = numbers_rdd.first()\n",
        "print(\"First element in RDD:\", initial_element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO12bRlIikw2",
        "outputId": "8b5c6eb8-a5b5-4b6a-94eb-62b37a047fe1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First element in RDD: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Use filter transformation to create a new RDD by selecting elements that are even\n",
        "even_numbers_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
        "print(\"Filtered RDD with even elements:\", even_numbers_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSiiRGHNimwF",
        "outputId": "f46ef3ea-bc94-4fee-d0b2-71611137fee2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered RDD with even elements: [2, 4, 6, 8, 10, 12, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Apply map transformation to each element in the RDD and return a new RDD with squares\n",
        "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
        "print(\"RDD with square of each element:\", squared_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY3mlfgFiosr",
        "outputId": "e6c47234-3ddd-4d67-93b3-6c13887fe540"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD with square of each element: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Aggregate all elements in the RDD using reduce action\n",
        "total_sum = numbers_rdd.reduce(lambda x, y: x + y)\n",
        "print(\"Sum of elements in RDD:\", total_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMiEGe7tiqzA",
        "outputId": "09328324-ea94-4deb-8390-435e1eca54ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of elements in RDD: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Save the RDD data as a text file\n",
        "numbers_rdd.saveAsTextFile(\"output_assignment_rdd\")\n"
      ],
      "metadata": {
        "id": "SrT8f1ntitaX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Take two new list RDDs and combine them with union transformation\n",
        "more_numbers_rdd = context.parallelize(range(16, 21))\n",
        "combined_rdd = numbers_rdd.union(more_numbers_rdd)\n",
        "print(\"Union of two RDDs:\", combined_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfcSSt-liuJ6",
        "outputId": "870a446e-b77f-49aa-952e-b9ac7504072f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Union of two RDDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Use cartesian transformation on defined list RDDs that returns a new list of ordered pairs\n",
        "paired_rdd = numbers_rdd.cartesian(more_numbers_rdd)\n",
        "print(\"Cartesian product of RDDs:\", paired_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGNBEU7pixQx",
        "outputId": "3304b0ff-4181-4aa4-c17c-0d4bd1bed484"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cartesian product of RDDs: [(1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (2, 16), (3, 16), (2, 17), (2, 18), (3, 17), (3, 18), (2, 19), (2, 20), (3, 19), (3, 20), (4, 16), (5, 16), (6, 16), (7, 16), (4, 17), (4, 18), (5, 17), (5, 18), (6, 17), (6, 18), (7, 17), (7, 18), (4, 19), (4, 20), (5, 19), (5, 20), (6, 19), (6, 20), (7, 19), (7, 20), (8, 16), (9, 16), (10, 16), (11, 16), (12, 16), (13, 16), (14, 16), (15, 16), (8, 17), (8, 18), (9, 17), (9, 18), (10, 17), (10, 18), (11, 17), (11, 18), (12, 17), (12, 18), (13, 17), (13, 18), (14, 17), (14, 18), (15, 17), (15, 18), (8, 19), (8, 20), (9, 19), (9, 20), (10, 19), (10, 20), (11, 19), (11, 20), (12, 19), (12, 20), (13, 19), (13, 20), (14, 19), (14, 20), (15, 19), (15, 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Create an RDD with Dictionary\n",
        "dict_rdd_alternate = context.parallelize([{\"name\": \"John\", \"age\": 30}, {\"name\": \"Doe\", \"age\": 40}, {\"name\": \"Eve\", \"age\": 25}])\n",
        "print(\"RDD with dictionary:\", dict_rdd_alternate.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNa5DAO7izq8",
        "outputId": "3e9a6d50-8d1c-4bdb-fb7e-d44b558fccb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD with dictionary: [{'name': 'John', 'age': 30}, {'name': 'Doe', 'age': 40}, {'name': 'Eve', 'age': 25}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Get unique value in the RDD as the key and its count as the value\n",
        "items_rdd = context.parallelize([\"pear\", \"grape\", \"pear\", \"plum\", \"grape\", \"pear\"])\n",
        "item_count_rdd = items_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "print(\"Unique values and their counts:\", item_count_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXg-WQYsi6vW",
        "outputId": "07e5bfe5-79b3-49d6-fae3-b090a19f781b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values and their counts: [('pear', 3), ('grape', 2), ('plum', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. # Create sample text files\n",
        "with open(\"file1.txt\", \"w\") as file:\n",
        "    file.write(\"This is line 1 of file 1\\n\")\n",
        "    file.write(\"This is line 2 of file 1\\n\")\n",
        "\n",
        "with open(\"file2.txt\", \"w\") as file:\n",
        "    file.write(\"This is line 1 of file 2\\n\")\n",
        "    file.write(\"This is line 2 of file 2\\n\")"
      ],
      "metadata": {
        "id": "G7Dp6nXGi8zX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadMultipleFiles\").getOrCreate()\n",
        "\n",
        "# Read the text files into an RDD\n",
        "rdd_from_files = spark.sparkContext.textFile(\"file1.txt,file2.txt\")\n",
        "print(rdd_from_files.collect())\n",
        "\n",
        "# Stop the SparkSession (optional, but recommended)\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soIxaRsvjJo4",
        "outputId": "9d3fe9fe-f2ae-4236-c5bc-095f0cb00d07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is line 1 of file 1', 'This is line 2 of file 1', 'This is line 1 of file 2', 'This is line 2 of file 2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Inspect the first 5 lines of an RDD\n",
        "# Cell 1: Read the text files into an RDD\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadMultipleFiles\").getOrCreate()\n",
        "\n",
        "# Read the text files into an RDD\n",
        "rdd_from_files = spark.sparkContext.textFile(\"file1.txt,file2.txt\")\n",
        "print(rdd_from_files.collect())\n",
        "\n",
        "# Stop the SparkSession (optional, but recommended)\n",
        "# Removing spark.stop() from here, keep the SparkSession alive for the next cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WytzTpRXjVvc",
        "outputId": "a7eea3af-c4e9-4c56-ab8a-8899856741cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is line 1 of file 1', 'This is line 2 of file 1', 'This is line 1 of file 2', 'This is line 2 of file 2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Create DataFrame and Dataset\n",
        "from pyspark.sql import SparkSession, Row #Import Row\n",
        "\n",
        "# Create a SparkSession if one doesn't exist or get the existing one\n",
        "spark = SparkSession.builder.appName(\"CreateDataframeDataset\").getOrCreate()\n",
        "\n",
        "# Creating DataFrame from RDD\n",
        "person_data = [Row(name=\"John\", age=30), Row(name=\"Doe\", age=40), Row(name=\"Eve\", age=25)]\n",
        "# Use spark instead of session\n",
        "data_frame = spark.createDataFrame(person_data)\n",
        "print(\"DataFrame:\")\n",
        "data_frame.show()\n",
        "\n",
        "# Ensure SparkSession and SparkContext are active and restart if necessary\n",
        "# Get the existing SparkContext or create a new one\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Creating sample numbers RDD\n",
        "numbers_rdd = sc.parallelize([1, 2, 3, 4, 5]) # Assign the RDD to numbers_rdd\n",
        "\n",
        "# You can now perform operations on the numbers_rdd, for example:\n",
        "print(\"Numbers RDD:\", numbers_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF52UtpajhAz",
        "outputId": "825a46fb-98e2-4c94-d0d5-7dab3767131a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame:\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|John| 30|\n",
            "| Doe| 40|\n",
            "| Eve| 25|\n",
            "+----+---+\n",
            "\n",
            "Numbers RDD: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Show difference between RDD, DataFrame, and Dataset\n",
        "# RDD: Basic distributed data processing API, untyped, allows any type of data\n",
        "print(\"RDD Example:\", numbers_rdd.collect())\n",
        "\n",
        "# DataFrame: Organized into named columns (structured data), similar to a table in SQL\n",
        "print(\"DataFrame Example:\")\n",
        "data_frame.show()\n",
        "\n",
        "# Dataset: Only available in Scala and Java APIs in Spark, combines RDD and DataFrame features with compile-time safety\n",
        "# In PySpark, DataFrames act as a replacement for Dataset\n",
        "print(\"Dataset Example in PySpark is represented using DataFrame:\")\n",
        "data_frame.show() # Changed dataset_alternative to data_frame\n",
        "\n",
        "# Stop the SparkContext\n",
        "context.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvrXF04ZkDrd",
        "outputId": "bb34dc77-d88a-4d5e-942f-6501178284d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Example: [1, 2, 3, 4, 5]\n",
            "DataFrame Example:\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|John| 30|\n",
            "| Doe| 40|\n",
            "| Eve| 25|\n",
            "+----+---+\n",
            "\n",
            "Dataset Example in PySpark is represented using DataFrame:\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|John| 30|\n",
            "| Doe| 40|\n",
            "| Eve| 25|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}